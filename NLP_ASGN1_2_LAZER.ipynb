{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c69fdb50-176d-44e1-8ce1-918a53738d96",
   "metadata": {},
   "source": [
    "# Task 1. Text Preprocessing with NLTK and spaCy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "77947d64-95b9-41cc-afef-4f01c4f99fb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import spacy\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5ebe47f5-71e7-4eb1-89d6-836afab5f782",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/lazer/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /Users/lazer/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /Users/lazer/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#NLTK resources\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "051eef1a-4d9f-49d1-a29f-6e683a4e6be6",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9a00f26e-10d9-4d7c-8a46-e3f5e7ea5ae1",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_paragraph = \"\"\"The vast Kazakh steppe stretched before them, an endless sea of grass.  \n",
    "The wind whispered tales of ancient khans and nomadic warriors.  \n",
    "A lone eagle soared overhead, a symbol of freedom and strength.  \n",
    "The sun beat down mercilessly, but the people endured, their spirits unbroken. \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "13133850-7f47-4cbc-b0b4-41b16e92676e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NLTK Processing\n",
    "nltk_tokens = word_tokenize(sample_paragraph.lower()) # Tokenization and lowercasing\n",
    "nltk_lemmatizer = WordNetLemmatizer()\n",
    "nltk_stopwords = set(stopwords.words('english'))\n",
    "\n",
    "nltk_lemmatized = [nltk_lemmatizer.lemmatize(token) for token in nltk_tokens if token.isalnum()] #Lemmatization and removing punctuation/non-alphanumeric\n",
    "\n",
    "nltk_filtered = [token for token in nltk_lemmatized if token not in nltk_stopwords] # Stopword removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7772b533-0ade-45fd-99db-c1288722aaef",
   "metadata": {},
   "outputs": [],
   "source": [
    "#spaCy Processing\n",
    "spacy_doc = nlp(sample_paragraph.lower())\n",
    "spacy_lemmatized = [token.lemma_ for token in spacy_doc if token.is_alpha]\n",
    "spacy_filtered = [token for token in spacy_lemmatized if token not in nlp.Defaults.stop_words]  # Corrected line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4126b39e-f681-456d-97ff-156598837911",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NLTK Tokens: ['the', 'vast', 'kazakh', 'steppe', 'stretched', 'before', 'them', ',', 'an', 'endless', 'sea', 'of', 'grass', '.', 'the', 'wind', 'whispered', 'tales', 'of', 'ancient', 'khans', 'and', 'nomadic', 'warriors', '.', 'a', 'lone', 'eagle', 'soared', 'overhead', ',', 'a', 'symbol', 'of', 'freedom', 'and', 'strength', '.', 'the', 'sun', 'beat', 'down', 'mercilessly', ',', 'but', 'the', 'people', 'endured', ',', 'their', 'spirits', 'unbroken', '.']\n",
      "NLTK Lemmatized: ['the', 'vast', 'kazakh', 'steppe', 'stretched', 'before', 'them', 'an', 'endless', 'sea', 'of', 'grass', 'the', 'wind', 'whispered', 'tale', 'of', 'ancient', 'khan', 'and', 'nomadic', 'warrior', 'a', 'lone', 'eagle', 'soared', 'overhead', 'a', 'symbol', 'of', 'freedom', 'and', 'strength', 'the', 'sun', 'beat', 'down', 'mercilessly', 'but', 'the', 'people', 'endured', 'their', 'spirit', 'unbroken']\n",
      "NLTK Filtered: ['vast', 'kazakh', 'steppe', 'stretched', 'endless', 'sea', 'grass', 'wind', 'whispered', 'tale', 'ancient', 'khan', 'nomadic', 'warrior', 'lone', 'eagle', 'soared', 'overhead', 'symbol', 'freedom', 'strength', 'sun', 'beat', 'mercilessly', 'people', 'endured', 'spirit', 'unbroken']\n",
      "\n",
      "spaCy Tokens (implicit): ['the', 'vast', 'kazakh', 'steppe', 'stretched', 'before', 'them', ',', 'an', 'endless', 'sea', 'of', 'grass', '.', ' \\n', 'the', 'wind', 'whispered', 'tales', 'of', 'ancient', 'khans', 'and', 'nomadic', 'warriors', '.', ' \\n', 'a', 'lone', 'eagle', 'soared', 'overhead', ',', 'a', 'symbol', 'of', 'freedom', 'and', 'strength', '.', ' \\n', 'the', 'sun', 'beat', 'down', 'mercilessly', ',', 'but', 'the', 'people', 'endured', ',', 'their', 'spirits', 'unbroken', '.']\n",
      "spaCy Lemmatized: ['the', 'vast', 'kazakh', 'steppe', 'stretch', 'before', 'they', 'an', 'endless', 'sea', 'of', 'grass', 'the', 'wind', 'whisper', 'tale', 'of', 'ancient', 'khan', 'and', 'nomadic', 'warrior', 'a', 'lone', 'eagle', 'soar', 'overhead', 'a', 'symbol', 'of', 'freedom', 'and', 'strength', 'the', 'sun', 'beat', 'down', 'mercilessly', 'but', 'the', 'people', 'endure', 'their', 'spirit', 'unbroken']\n",
      "spaCy Filtered: ['vast', 'kazakh', 'steppe', 'stretch', 'endless', 'sea', 'grass', 'wind', 'whisper', 'tale', 'ancient', 'khan', 'nomadic', 'warrior', 'lone', 'eagle', 'soar', 'overhead', 'symbol', 'freedom', 'strength', 'sun', 'beat', 'mercilessly', 'people', 'endure', 'spirit', 'unbroken']\n"
     ]
    }
   ],
   "source": [
    "print(\"NLTK Tokens:\", nltk_tokens)\n",
    "print(\"NLTK Lemmatized:\", nltk_lemmatized)\n",
    "print(\"NLTK Filtered:\", nltk_filtered)\n",
    "\n",
    "print(\"\\nspaCy Tokens (implicit):\", [token.text for token in spacy_doc]) # spaCy tokenizes as part of the nlp pipeline\n",
    "print(\"spaCy Lemmatized:\", spacy_lemmatized)\n",
    "print(\"spaCy Filtered:\", spacy_filtered)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b6829e5-a2b9-44be-b181-2eec0f221c02",
   "metadata": {},
   "source": [
    "# Task 2. Named Entity Recognition (NER) with spaCy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3528a026-0f56-4a94-a13a-02b1880525b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy import displacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f02894da-8d22-4c00-bb6f-16869f8e086a",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2ccd807e-b98e-46ed-bfd3-dbbea57dbd9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_text = \"\"\"Beshbarmak, a traditional dish made with boiled meat and noodles, is a staple of Kazakh cuisine.  \n",
    "Yurts, portable dwellings used by nomadic people, are a common sight in rural areas.  \n",
    "The Baikonur Cosmodrome, from which Yuri Gagarin launched into space, is located in Kazakhstan.  \n",
    "Kazakhstan gained independence in 1991.  \n",
    "The capital city, Astana, is a modern metropolis. \"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3b4a9620-191c-48b7-9542-c9549c04c5c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(sample_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ecb742ea-8845-4f5c-be4c-54d9a1cb6dd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Beshbarmak ORG\n",
      "Kazakh ORG\n",
      "Yuri Gagarin PERSON\n",
      "Kazakhstan GPE\n",
      "Kazakhstan GPE\n",
      "1991 DATE\n",
      "Astana GPE\n"
     ]
    }
   ],
   "source": [
    "# Extract Named Entities\n",
    "for ent in doc.ents:\n",
    "    print(ent.text, ent.label_) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d6ee8908-c75c-45ea-b2cd-e78c52564b05",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">\n",
       "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Beshbarmak\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
       "</mark>\n",
       ", a traditional dish made with boiled meat and noodles, is a staple of \n",
       "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Kazakh\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
       "</mark>\n",
       " cuisine.  <br>Yurts, portable dwellings used by nomadic people, are a common sight in rural areas.  <br>The Baikonur Cosmodrome, from which \n",
       "<mark class=\"entity\" style=\"background: #aa9cfc; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Yuri Gagarin\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">PERSON</span>\n",
       "</mark>\n",
       " launched into space, is located in \n",
       "<mark class=\"entity\" style=\"background: #feca74; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Kazakhstan\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">GPE</span>\n",
       "</mark>\n",
       ".  <br>\n",
       "<mark class=\"entity\" style=\"background: #feca74; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Kazakhstan\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">GPE</span>\n",
       "</mark>\n",
       " gained independence in \n",
       "<mark class=\"entity\" style=\"background: #bfe1d9; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    1991\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">DATE</span>\n",
       "</mark>\n",
       ".  <br>The capital city, \n",
       "<mark class=\"entity\" style=\"background: #feca74; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Astana\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">GPE</span>\n",
       "</mark>\n",
       ", is a modern metropolis. </div></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#visualize them\n",
    "displacy.render(doc, style=\"ent\", jupyter=True) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a388e732-edd9-4067-b24a-c666c39e240b",
   "metadata": {},
   "source": [
    "# Task 3. Text Vectorization using Transformers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c83acd2c-e49f-4e3e-acc1-15f8e0247b90",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fa513fdc-ad1d-4975-8512-96df5a85ab18",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"sentence-transformers/all-mpnet-base-v2\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModel.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1db8195a-f3ee-4d00-bb27-0bdceb7edf21",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = [\n",
    "    \"The Altai Mountains, with their breathtaking scenery, attract tourists from around the world.\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cf6af719-1e07-4156-8db5-7d2f6dc6975c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenize and encode\n",
    "encoded_input = tokenizer(sentence, padding=True, truncation=True, return_tensors='pt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d3ce2b48-4e38-4ec9-a6cb-9be1fb1cf1e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mean Pooling Function\n",
    "def mean_pooling(model_output, attention_mask):\n",
    "    token_embeddings = model_output[0]\n",
    "    input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
    "    sum_embeddings = torch.sum(token_embeddings * input_mask_expanded, 1)\n",
    "    sum_mask = torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n",
    "    mean_embeddings = sum_embeddings / sum_mask\n",
    "    return mean_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1ed0faed-8616-4825-9c75-f71d44498723",
   "metadata": {},
   "outputs": [],
   "source": [
    "# embedding get\n",
    "with torch.no_grad():\n",
    "    outputs = model(**encoded_input)\n",
    "    hidden_states = outputs.last_hidden_state\n",
    "    sentence_embedding = torch.mean(hidden_states, dim=1)\n",
    "    sentence_embedding_mean_pooled = mean_pooling(outputs, encoded_input['attention_mask'])  # Mean-pooled sentence embedding\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "bce78371-0a82-4f3e-b3d0-699e7fdbb8ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of hidden states (word embeddings): torch.Size([1, 20, 768])\n",
      "Hidden size (embedding dimension): 768\n",
      "Shape of sentence embedding (averaged): torch.Size([1, 768])\n",
      "Shape of sentence embedding (mean-pooled): torch.Size([1, 768])\n"
     ]
    }
   ],
   "source": [
    "print(\"Shape of hidden states (word embeddings):\", hidden_states.shape)\n",
    "print(\"Hidden size (embedding dimension):\", model.config.hidden_size)\n",
    "print(\"Shape of sentence embedding (averaged):\", sentence_embedding.shape)\n",
    "print(\"Shape of sentence embedding (mean-pooled):\", sentence_embedding_mean_pooled.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "28dbbf77-509e-4a2c-96f9-4d9571263de9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding of the first word: torch.Size([768])\n"
     ]
    }
   ],
   "source": [
    "# Example: third word embedding\n",
    "third_word_embedding = hidden_states[0, 3, :]\n",
    "print(\"Embedding of the first word:\", third_word_embedding.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b87c918-f923-4375-96b4-e52fc7825da8",
   "metadata": {},
   "source": [
    "# Task 4, Sentiment Analysis with Transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "247647dc-b136-47b9-ab20-126cc8c70a7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import  pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d9858961-c9e0-46c6-b2b5-fafe82592880",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at cardiffnlp/twitter-roberta-base-sentiment-latest were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use mps:0\n"
     ]
    }
   ],
   "source": [
    "classifier = pipeline(\"sentiment-analysis\", model=\"cardiffnlp/twitter-roberta-base-sentiment-latest\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b5552d8e-f240-4c2b-90fd-1ae46e6eca9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = [\n",
    "     \"This movie was absolutely fantastic!\",\n",
    "    \"The food was terrible and the service was slow.\",\n",
    "    \"The book was okay, but I didn't love it.\",\n",
    "    \"I had a neutral experience at the restaurant.\",\n",
    "    \"The performance was breathtaking and emotionally moving.\",\n",
    "    \"The actor's performance was mediocre and unconvincing.\",\n",
    "    \"The plot was confusing and poorly written.\",\n",
    "    \"The special effects were stunning and visually impressive.\",\n",
    "    \"The weather today is unremarkable.\",\n",
    "    \"The color of the walls is beige.\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2fe2b9aa-d379-4263-be59-0fb7ce72d384",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sentiment Analysis\n",
    "results = classifier(sentences)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b03ddbc6-b1a7-4cae-8b59-104e29a98022",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence 1: This movie was absolutely fantastic!\n",
      "Sentiment: positive\n",
      "Score: 0.9820\n",
      "--------------------------------------------------\n",
      "Sentence 2: The food was terrible and the service was slow.\n",
      "Sentiment: negative\n",
      "Score: 0.9555\n",
      "--------------------------------------------------\n",
      "Sentence 3: The book was okay, but I didn't love it.\n",
      "Sentiment: negative\n",
      "Score: 0.8365\n",
      "--------------------------------------------------\n",
      "Sentence 4: I had a neutral experience at the restaurant.\n",
      "Sentiment: neutral\n",
      "Score: 0.6010\n",
      "--------------------------------------------------\n",
      "Sentence 5: The performance was breathtaking and emotionally moving.\n",
      "Sentiment: positive\n",
      "Score: 0.9801\n",
      "--------------------------------------------------\n",
      "Sentence 6: The actor's performance was mediocre and unconvincing.\n",
      "Sentiment: negative\n",
      "Score: 0.9037\n",
      "--------------------------------------------------\n",
      "Sentence 7: The plot was confusing and poorly written.\n",
      "Sentiment: negative\n",
      "Score: 0.9194\n",
      "--------------------------------------------------\n",
      "Sentence 8: The special effects were stunning and visually impressive.\n",
      "Sentiment: positive\n",
      "Score: 0.9820\n",
      "--------------------------------------------------\n",
      "Sentence 9: The weather today is unremarkable.\n",
      "Sentiment: negative\n",
      "Score: 0.7206\n",
      "--------------------------------------------------\n",
      "Sentence 10: The color of the walls is beige.\n",
      "Sentiment: neutral\n",
      "Score: 0.8414\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "for i, sentence in enumerate(sentences):\n",
    "    print(f\"Sentence {i+1}: {sentence}\")\n",
    "    print(f\"Sentiment: {results[i]['label']}\")\n",
    "    print(f\"Score: {results[i]['score']:.4f}\")\n",
    "    print(\"-\" * 50)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0bd1feb-c222-4881-beb2-aad9c8692f4a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (spaCy)",
   "language": "python",
   "name": "spacy_venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
